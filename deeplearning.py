# -*- coding: utf-8 -*-
"""DeepLearning-course.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LCeBJdmTAgHmCOM2eSct7XtH-QnWYhb7

Data preparation and  preprocessing
"""

import numpy as np
from random import randint
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler

train_labels =[]
train_samples = []

"""Example Data:


*   An Experimental Drug was tested  on individuals from ages 13 to 100 in   clinical trials.
*   The trial had 2100 participants. Half were 65 years or older.


*   Around 95% of patients 65 or older experienced side effects.
*   Around 95% of patients under 65 experienced no side effects.




"""

for i in range(50):
  # The ~5% of younger individuals who did experience side effects.
  random_younger = randint(13,64)
  train_samples.append(random_younger)
  train_labels.append(1)

  # The ~5% of older individuals who did not experience side effects.
  random_older = randint(65,100)
  train_samples.append(random_older)
  train_labels.append(0)

for i in range(1000):
  # The ~95% of younger individual who did not experience side effects..
  
  random_younger = randint(13,64)
  train_samples.append(random_younger)
  train_labels.append(0)

  # The ~95% of older individuals who did  experience side effects.

  random_older = randint(65,100)
  train_samples.append(random_older)
  train_labels.append(1)

for i in train_samples:
  print(i)

for i in train_labels:
  print(i)

# take data and covert into an numpy array...

train_labels = np.array(train_labels)
train_samples = np.array(train_samples)

train_labels, train_samples = shuffle(train_labels,train_samples)

# Normalize and standardize the data to perform deep learning efficiently...
scaler = MinMaxScaler(feature_range=(0,1))
scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))

for i in scaled_train_samples:
  print(i)

"""Tensorflow Sequential Model using keras library with tensorflow integration """

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy

model = Sequential([
                    Dense(units=16, input_shape = (1,), activation='relu'),
                    Dense(units=32, activation='relu'),
                    Dense(units=2,activation='softmax')
])

model.summary()

model.compile(optimizer=Adam(learning_rate=0.0001), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(x = scaled_train_samples, y=train_labels, validation_split=0.1,batch_size=10,epochs=30,shuffle = True,verbose=2)

# Now we have to create test data, the process is same as trained data...

test_labels =[]
test_samples = []

for i in range(50):
  # The ~5% of younger individuals who did experience side effects.
  random_younger = randint(13,64)
  test_samples.append(random_younger)
  test_labels.append(1)

  # The ~5% of older individuals who did not experience side effects.
  random_older = randint(65,100)
  test_samples.append(random_older)
  test_labels.append(0)

for i in range(1000):
  # The ~95% of younger individual who did not experience side effects..
  
  random_younger = randint(13,64)
  test_samples.append(random_younger)
  test_labels.append(0)

  # The ~95% of older individuals who did  experience side effects.

  random_older = randint(65,100)
  test_samples.append(random_older)
  test_labels.append(1)

# take data and covert into an numpy array...

test_labels = np.array(train_labels)
test_samples = np.array(train_samples)

test_labels, train_samples = shuffle(train_labels,train_samples)

# Normalize and standardize the data to perform deep learning efficiently...
scaler = MinMaxScaler(feature_range=(0,1))
scaled_test_samples = scaler.fit_transform(train_samples.reshape(-1,1))

# use test data on model which we creted before...
predictions = model.predict(x=scaled_test_samples,batch_size=10,verbose=0)

for i in predictions:
  print(i)

rounded_predictions = np.argmax(predictions, axis=1)

for i in rounded_predictions:
  print(i)

# 47.33 -> next episode...

"""Confusion Matrix.."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.metrics import confusion_matrix
import itertools
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)

def plot_confusion_matrix(cm,classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):
  plt.imshow(cm,interpolation='nearest',cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes,rotation=45)
  plt.yticks(tick_marks,classes)

  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:np.newaxis]
    print('Normalized Confusion matrix')
  else:
    print('Confusion matrix, without normalization')

  print(cm)
  thresh = cm.max()/2.
  for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
    plt.text(j, i, cm[i, j],horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel(' True label')
    plt.xlabel(' Predicted label')

cm_plot_labels = [ ' no_side_effects','had_side_effects']
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title="Confusion Matrix")

"""Save and load keras seuential model...

"""

import os.path
if os.path.isfile('path') is False:
  model.save('same path as above')

from tensorflow.keras.models import load_model
new_model = load_model('path to saved model')

model.get_weights()

model.optimizer

"""2. model.to_json()


"""

#save as JSON
json_string = model.to_json()
#save as YAML
#yamL_string  = model.to_yamL()

json_string

#model reconstruction from JSON..

from tensorflow.keras.models import model_from_json
model_architecture = model_from_json(json_string)

#model reconstruction  from YAML
#from tensorflow.keras.models import model_from_yaml
#model= model_from_yaml(yaml_string)

model_architecture.summary()

"""3. model.save_weight()
to save the weigths

Training on Images -CNN..
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from tensorflow.keras.layers import Activation,Dense,Flatten,BatchNormalization,Conv2D,MaxPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
import itertools
import os
import shutil
import random
import glob
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter(action='ignore',category=FutureWarning)
# %matplotlib inline

"""Data Preparation.."""

# Organize data into train, valid ,test dir

